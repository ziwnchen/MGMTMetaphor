{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference of wsd classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zfs/projects/faculty/amirgo-management/.pytorch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertTokenizerFast, BertConfig,AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # Import the functional module to apply softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from  tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "class ManageDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, labels, target_char_spans):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.char_spans = target_char_spans\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the sentence into BERT tokens with offset mappings (fast tokenizer)\n",
    "        inputs = self.tokenizer(\n",
    "            self.sentences[idx],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=256,\n",
    "            return_offsets_mapping=True # return tuple indicating the sub-token's start position\n",
    "        )\n",
    "\n",
    "        # Generate the manag_mask\n",
    "        manag_mask = self._get_manag_mask(\n",
    "            self.sentences[idx],\n",
    "            inputs[\"input_ids\"][0],\n",
    "            inputs[\"offset_mapping\"][0],\n",
    "            self.char_spans[idx]\n",
    "        )\n",
    "\n",
    "        # Return tokens' embeddings and the label\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"][0],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"][0],\n",
    "            \"manag_mask\": manag_mask,\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def _get_manag_mask(self, sentence, input_ids, offset_mapping, target_char_span):\n",
    "        # Initialize manag_mask\n",
    "        manag_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "        # Iterate over BERT tokens and align with target word's character span\n",
    "        for i, (start, end) in enumerate(offset_mapping):\n",
    "            if start == 0 and end == 0:\n",
    "                continue  # Skip special tokens like [CLS], [SEP], [PAD]\n",
    "            if (start >= target_char_span[0] and start < target_char_span[1]) or \\\n",
    "               (end > target_char_span[0] and end <= target_char_span[1]) or \\\n",
    "               (start <= target_char_span[0] and end >= target_char_span[1]):\n",
    "                manag_mask[i] = True\n",
    "        return manag_mask\n",
    "\n",
    "class BERTWSDModel(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', num_labels=2):\n",
    "        super(BERTWSDModel, self).__init__()\n",
    "        # Load pre-trained BERT model\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        # Save the configuration\n",
    "        self.config = self.bert.config\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, manag_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  # (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Apply manag_mask to get embeddings of target tokens\n",
    "        manag_mask_expanded = manag_mask.unsqueeze(-1).expand(last_hidden_state.size())\n",
    "        target_embeddings = last_hidden_state * manag_mask_expanded.float()\n",
    "\n",
    "        # Compute average embeddings for each sample in the batch\n",
    "        token_counts = manag_mask.sum(dim=1).unsqueeze(-1)  # (batch_size, 1)\n",
    "        # Avoid division by zero\n",
    "        token_counts[token_counts == 0] = 1\n",
    "        avg_embeddings = target_embeddings.sum(dim=1) / token_counts  # (batch_size, hidden_size)\n",
    "\n",
    "        # Apply dropout\n",
    "        pooled_output = self.dropout(avg_embeddings)\n",
    "\n",
    "        # Get logits from classifier\n",
    "        logits = self.classifier(pooled_output)  # (batch_size, num_labels)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        torch.save(self.state_dict(), os.path.join(save_directory, 'pytorch_model.bin'))\n",
    "        self.config.save_pretrained(save_directory)\n",
    "        print(f\"Model saved to {save_directory}\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, load_directory):\n",
    "        # Load the model configuration\n",
    "        config = BertModel.from_pretrained(load_directory).config\n",
    "        # Initialize the model\n",
    "        model = cls(bert_model_name=load_directory)\n",
    "        # Load the model state dict\n",
    "        model_load_path = os.path.join(load_directory, 'pytorch_model.bin')\n",
    "        if torch.cuda.is_available():\n",
    "            model.load_state_dict(torch.load(model_load_path))\n",
    "            model = model.to('cuda')\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(model_load_path, map_location=torch.device('cpu')))\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3401315/2344958368.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_load_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTWSDModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your model and tokenizer\n",
    "save_directory = \"/zfs/projects/faculty/amirgo-management/BERT/WSD_Oct21/\"\n",
    "loaded_tokenizer = BertTokenizerFast.from_pretrained(save_directory)\n",
    "loaded_model = BERTWSDModel.from_pretrained(save_directory)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(sentences, char_spans, model, tokenizer, batch_size=10):\n",
    "    dataset = ManageDataset(tokenizer, sentences, [0]*len(sentences), char_spans)  # Dummy labels just for data processing\n",
    "    loader = DataLoader(dataset, batch_size)  # Set batch size according to your needs\n",
    "\n",
    "    model.eval()\n",
    "    pred_labels = []\n",
    "    confidences = []  # To store prediction confidences\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Inferencing\", unit=\"batch\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            manag_mask = batch[\"manag_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask, manag_mask)\n",
    "\n",
    "            # Convert logits to probabilities using softmax\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            # Get the predicted labels and their corresponding confidences\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            conf = probs[range(probs.shape[0]), preds].tolist()  # Get the confidence of the predicted class for each sample\n",
    "\n",
    "            pred_labels.extend(preds.tolist())\n",
    "            confidences.extend(conf)\n",
    "\n",
    "    return pred_labels, confidences  # Return both predicted labels and their confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0: \"Intransitive\", 1: \"Transitive\"}\n",
    "\n",
    "def get_word_char_spans(sentence, words):\n",
    "    char_spans = []\n",
    "    current_pos = 0\n",
    "    for word in words:\n",
    "        pattern = re.escape(word)\n",
    "        match = re.search(pattern, sentence[current_pos:])\n",
    "        if match is None:\n",
    "            raise ValueError(f\"Word '{word}' not found in sentence.\")\n",
    "        start_idx = current_pos + match.start()\n",
    "        end_idx = current_pos + match.end()\n",
    "        char_spans.append((start_idx, end_idx))\n",
    "        current_pos = end_idx\n",
    "    return char_spans\n",
    "\n",
    "def infer_individual_sentence(sentence, target_word, model=loaded_model, tokenizer=loaded_tokenizer):\n",
    "    char_span = get_word_char_spans(sentence, [target_word])[0]\n",
    "    pred_labels,confidences = infer([sentence],[char_span], model, tokenizer)\n",
    "    print(label_dict[pred_labels[0]], confidences[0])\n",
    "    return char_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 100%|██████████| 1/1 [00:00<00:00,  9.08batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intransitive 0.9905189871788025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([29, 35])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# individual prediction\n",
    "test = \"I don't think I could really manage to do that.\"\n",
    "infer_individual_sentence(test, 'manage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 100%|██████████| 1/1 [00:00<00:00,  5.49batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intransitive 0.9639004468917847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# individual prediction\n",
    "test = \"We barely manage this year.\"\n",
    "infer_individual_sentence(test, 'manage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 100%|██████████| 1/1 [00:00<00:00,  5.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intransitive 0.9039086699485779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# individual prediction\n",
    "test = \"I don't think I could really manage being married to your mother anymore.\"\n",
    "infer_individual_sentence(test, 'manage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 100%|██████████| 1/1 [00:00<00:00,  5.21batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intransitive 0.5622024536132812\n"
     ]
    }
   ],
   "source": [
    "test = \"For one year that remained they could manage; if george wasn't willing to try, it wasn't money that was stopping him., it was the idea of marriage itself.\"\n",
    "infer_individual_sentence(test, 'manage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 100%|██████████| 1/1 [00:00<00:00,  4.99batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitive 0.8727024793624878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test = \"A wealthy widow and a unmanageable daughter.\"\n",
    "infer_individual_sentence(test, 'unmanageable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 100%|██████████| 1/1 [00:00<00:00,  6.78batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitive 0.996315062046051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# not the perfect kind of training dataset, but I don't think it's a big problem as the major trend should be captured\n",
    "test = \"The doctor know how to manage patients with mental health issues.\"\n",
    "infer_individual_sentence(test, 'manage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file load examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
