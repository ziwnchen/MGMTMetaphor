{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference of PB classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zfs/projects/faculty/amirgo-management/.pytorch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "import transformers\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManageDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, primary_labels, subcategory_labels, target_char_spans):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = sentences\n",
    "        self.primary_labels = primary_labels  # List of primary category labels\n",
    "        self.subcategory_labels = subcategory_labels  # List of subcategory labels\n",
    "        self.char_spans = target_char_spans  # List of character spans for target words\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the sentence into BERT tokens with offset mappings\n",
    "        inputs = self.tokenizer(\n",
    "            self.sentences[idx],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=256,\n",
    "            return_offsets_mapping=True  # Return offset mappings for sub-token positions\n",
    "        )\n",
    "\n",
    "        # Generate the manag_mask\n",
    "        manag_mask = self._get_manag_mask(\n",
    "            self.sentences[idx],\n",
    "            inputs[\"input_ids\"][0],\n",
    "            inputs[\"offset_mapping\"][0],\n",
    "            self.char_spans[idx]\n",
    "        )\n",
    "\n",
    "        # Return tokens' embeddings and the labels\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"][0],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"][0],\n",
    "            \"manag_mask\": manag_mask,\n",
    "            \"primary_labels\": torch.tensor(self.primary_labels[idx], dtype=torch.long),\n",
    "            \"subcategory_labels\": torch.tensor(self.subcategory_labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def _get_manag_mask(self, sentence, input_ids, offset_mapping, target_char_span):\n",
    "        # Initialize manag_mask\n",
    "        manag_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "        # Iterate over BERT tokens and align with target word's character span\n",
    "        for i, (start, end) in enumerate(offset_mapping):\n",
    "            if start == 0 and end == 0:\n",
    "                continue  # Skip special tokens like [CLS], [SEP], [PAD]\n",
    "            if (start >= target_char_span[0] and start < target_char_span[1]) or \\\n",
    "               (end > target_char_span[0] and end <= target_char_span[1]) or \\\n",
    "               (start <= target_char_span[0] and end >= target_char_span[1]):\n",
    "                manag_mask[i] = True\n",
    "        return manag_mask\n",
    "\n",
    "\n",
    "class BERTClassificationModel(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', num_primary_labels=3, num_subcategory_labels=10):\n",
    "        super(BERTClassificationModel, self).__init__()\n",
    "        # Load pre-trained BERT model\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        # Classification heads for primary and subcategories\n",
    "        self.primary_classifier = nn.Linear(self.bert.config.hidden_size, num_primary_labels)\n",
    "        self.subcategory_classifier = nn.Linear(self.bert.config.hidden_size, num_subcategory_labels)\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        # Save the configuration\n",
    "        self.config = self.bert.config\n",
    "        self.num_primary_labels = num_primary_labels\n",
    "        self.num_subcategory_labels = num_subcategory_labels\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, manag_mask):\n",
    "        # Pass inputs through BERT model\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  # (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Apply manag_mask to get embeddings of target tokens\n",
    "        manag_mask_expanded = manag_mask.unsqueeze(-1).expand(last_hidden_state.size())\n",
    "        target_embeddings = last_hidden_state * manag_mask_expanded.float()\n",
    "\n",
    "        # Compute average embeddings for each sample in the batch\n",
    "        token_counts = manag_mask.sum(dim=1).unsqueeze(-1)  # (batch_size, 1)\n",
    "        # Avoid division by zero\n",
    "        token_counts[token_counts == 0] = 1\n",
    "        avg_embeddings = target_embeddings.sum(dim=1) / token_counts  # (batch_size, hidden_size)\n",
    "\n",
    "        # Apply dropout\n",
    "        pooled_output = self.dropout(avg_embeddings)\n",
    "\n",
    "        # Get logits from classifiers\n",
    "        primary_logits = self.primary_classifier(pooled_output)  # (batch_size, num_primary_labels)\n",
    "        subcategory_logits = self.subcategory_classifier(pooled_output)  # (batch_size, num_subcategory_labels)\n",
    "\n",
    "        return primary_logits, subcategory_logits\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        # Save model state dict\n",
    "        torch.save(self.state_dict(), os.path.join(save_directory, 'pytorch_model.bin'))\n",
    "        # Save configuration with label information\n",
    "        self.config.num_primary_labels = self.num_primary_labels\n",
    "        self.config.num_subcategory_labels = self.num_subcategory_labels\n",
    "        self.config.save_pretrained(save_directory)\n",
    "        print(f\"Model saved to {save_directory}\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, load_directory):\n",
    "        # Load the model configuration\n",
    "        config = BertModel.from_pretrained(load_directory).config\n",
    "        # Get the number of labels from the saved config\n",
    "        num_primary_labels = config.num_primary_labels\n",
    "        num_subcategory_labels = config.num_subcategory_labels\n",
    "        # Initialize the model\n",
    "        model = cls(\n",
    "            bert_model_name=load_directory,\n",
    "            num_primary_labels=num_primary_labels,\n",
    "            num_subcategory_labels=num_subcategory_labels\n",
    "        )\n",
    "        # Load the model state dict\n",
    "        model_load_path = os.path.join(load_directory, 'pytorch_model.bin')\n",
    "        if torch.cuda.is_available():\n",
    "            model.load_state_dict(torch.load(model_load_path))\n",
    "            model = model.to('cuda')\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(model_load_path, map_location=torch.device('cpu')))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_743177/2797497108.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_load_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClassificationModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (primary_classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (subcategory_classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"/zfs/projects/faculty/amirgo-management/BERT/PB_MultiClass_Full_Oct30/\"\n",
    "model = BERTClassificationModel.from_pretrained(save_directory)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(save_directory)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(sentences, char_spans, model, tokenizer, batch_size=10):\n",
    "    dataset = ManageDataset(tokenizer, sentences, [0]*len(sentences), [0]*len(sentences), char_spans)\n",
    "    loader = DataLoader(dataset, batch_size)  # Set batch size according to your needs\n",
    "\n",
    "    model.eval()\n",
    "    pred_primary_labels = []\n",
    "    pred_subcategory_labels = []\n",
    "    primary_confidences = []\n",
    "    subcategory_confidences = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            manag_mask = batch['manag_mask'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            primary_logits, subcategory_logits = model(input_ids, attention_mask, manag_mask)\n",
    "\n",
    "            # Convert logits to probabilities using softmax\n",
    "            primary_probs =torch.softmax(primary_logits, dim=1)\n",
    "            subcategory_probs = torch.softmax(subcategory_logits, dim=1)\n",
    "\n",
    "            # Get the predicted labels (indices of max probabilities)\n",
    "            primary_preds = torch.argmax(primary_probs, dim=1)\n",
    "            subcategory_preds = torch.argmax(subcategory_probs, dim=1)\n",
    "\n",
    "            primary_confidence = torch.max(primary_probs, dim=1).values\n",
    "            subcategory_confidence = torch.max(subcategory_probs, dim=1).values\n",
    "\n",
    "            # Append predictions and confidences to the lists\n",
    "            pred_primary_labels.extend(primary_preds.cpu().numpy())\n",
    "            pred_subcategory_labels.extend(subcategory_preds.cpu().numpy())\n",
    "            primary_confidences.extend(primary_confidence.cpu().numpy())\n",
    "            subcategory_confidences.extend(subcategory_confidence.cpu().numpy())\n",
    "\n",
    "    return pred_primary_labels, pred_subcategory_labels, primary_confidences, subcategory_confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label cleaning\n",
    "secondary_map_num = {\"Others\": 0,\n",
    "                     \"Financials\": 1,\n",
    "                     \"Emotion and subjective experiences\": 2,\n",
    "                     \"Human body\": 3,\n",
    "                     \"Household\": 4,\n",
    "                     \"Family\": 5,\n",
    "                     \"Time\": 6,\n",
    "                     \"Romantic relationships\": 7,\n",
    "                     \"Friendship\": 8,\n",
    "                     \"Business Operations\": 9}\n",
    "\n",
    "primary_map_num = {'Personal': 0, 'Business and Professional': 1, 'Others': 2}\n",
    "reverse_primary_map = {v: k for k, v in primary_map_num.items()}\n",
    "reverse_secondary_map_num = {v: k for k, v in secondary_map_num.items()}\n",
    "\n",
    "\n",
    "def get_word_char_spans(sentence, words):\n",
    "    char_spans = []\n",
    "    current_pos = 0\n",
    "    for word in words:\n",
    "        pattern = re.escape(word)\n",
    "        match = re.search(pattern, sentence[current_pos:])\n",
    "        if match is None:\n",
    "            raise ValueError(f\"Word '{word}' not found in sentence.\")\n",
    "        start_idx = current_pos + match.start()\n",
    "        end_idx = current_pos + match.end()\n",
    "        char_spans.append((start_idx, end_idx))\n",
    "        current_pos = end_idx\n",
    "    return char_spans\n",
    "\n",
    "def infer_individual_sentence(sentence, target_word):\n",
    "    char_span = get_word_char_spans(sentence, [target_word])[0]\n",
    "    pred_primary_labels, pred_subcategory_labels, primary_confidences, subcategory_confidences = infer([sentence],[char_span],model, tokenizer)\n",
    "    print(reverse_primary_map[pred_primary_labels[0]], primary_confidences[0])\n",
    "    print(reverse_secondary_map_num[pred_subcategory_labels[0]], subcategory_confidences[0])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business and Professional 0.9943504\n",
      "Business Operations 0.9914539\n"
     ]
    }
   ],
   "source": [
    "# individual prediction\n",
    "test = \"the manager is good at managing his children, but he doesn't know how to manage his employee.\"\n",
    "infer_individual_sentence(test, 'manager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business and Professional 0.9757144\n",
      "Business Operations 0.9939633\n"
     ]
    }
   ],
   "source": [
    "test = \"Joe is also a good manager.\"\n",
    "infer_individual_sentence(test, 'manager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Others 0.99932766\n",
      "Others 0.9985071\n"
     ]
    }
   ],
   "source": [
    "test = \"I manage, with all my effort, to smile at their customers.\"\n",
    "infer_individual_sentence(test, 'manage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business and Professional 0.9210463\n",
      "Business Operations 0.85461885\n"
     ]
    }
   ],
   "source": [
    "test = \"She still need to work on management skills.\"\n",
    "infer_individual_sentence(test, 'management')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business and Professional 0.999788\n",
      "Business Operations 0.9992181\n"
     ]
    }
   ],
   "source": [
    "test = \"The manager, foreseeing a thining theatre, gave us free admission.\"\n",
    "infer_individual_sentence(test, 'manager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal 0.9624546\n",
      "Romantic relationships 0.94145226\n"
     ]
    }
   ],
   "source": [
    "test = \"Managing sexuallity is a difficult task for many people.\"\n",
    "infer_individual_sentence(test, 'Managing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal 0.99833554\n",
      "Emotion and subjective experiences 0.99834096\n"
     ]
    }
   ],
   "source": [
    "test = \"She manages a weak smile, but her eyes are full of tears.\"\n",
    "infer_individual_sentence(test, 'manages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal 0.9860299\n",
      "Human body 0.98886174\n"
     ]
    }
   ],
   "source": [
    "# not the perfect kind of training dataset, but I don't think it's a big problem as the major trend should be captured\n",
    "test = \"The doctor know how to manage patients with mental health issues.\"\n",
    "infer_individual_sentence(test, 'manage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
